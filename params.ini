[runtime]
; emcee fisher test importance multinest
sampler = importance
;root = ${COSMOSIS_SRC_DIR}

[importance]
; The output of a previous cosmosis run that you want to combine likelihoods with 
input = chains/2pt_NG.fits_d_w_chain_trimmed.txt
; The number of likelihood computations performed between each line of output 
nstep = 128
; In computing the importance of the input samples, we can add the new
; likelihood to the prior ones (instead of replacing them entirely).
add_to_likelihood = T

[test]
save_dir = test
;graph = pipeline.dot

[fisher]
step_size = 0.02

[emcee]
; The emcee sampler uses the concept of walkers, a collection
; of live points.  Sampling is done along lines that connect
; pairs of walkers.  The number of walkers must be at least
; 2*nparam + 1, but in general more than that usually works
; better.
walkers = 64
; This many samples is overkill, just to make the plots
; look a lot nicer
samples = 500
; This is the interval at which convergence diagnostics
; are performed
nsteps = 100

[multinest]
;wrapped_params = cosmological_parameters--omega_b   cosmological_parameters--h0
max_iterations=50000
;multinest_outfile_root=mn_%(RUN_NAME)s
multinest_outfile_root=o2_
resume=F
; from Joe:- For a quick run:
;live_points=250
;efficiency=0.8
;tolerance=0.1
;constant_efficiency=F
;    ~ 140k evaluations
;    ~ 9k independent samples
;    Few enough samples that the plots will be too scrappy to use in publications.
;    Estimated mean and covmat will still be good to a few percent
;    Evidence will be a bit off compared to other estimates.
;- Suggested standard run:
;live_points=500
;efficiency=0.3
;tolerance=0.1
;constant_efficiency=F
;    ~ 350k evaluations
;    ~ 20k independent samples
;    Plots should be fine with some small smoothing or Schuhman's transformation.
;    Reasonable evidence
;- A mega-run:
live_points=1000
efficiency=0.05
tolerance=0.1
constant_efficiency=T
;    ~ 1M evaluations
;    ~ 40k independent samples
;    Switching on constant_efficiency speeds up from ~1M samples to ~600k
;    Should only need one or two of these for comparison.



[pymc]
nsteps = 500
samples = 30000
adaptive_mcmc = True
; covmat = covmat6.txt
; In adaptive MCMC the proposal covariance
; is gradually tuned whilst maintaining the overall
; chain convergence properties.  You could set:
; adaptive_mcmc = True
; and you would not need a covariance matrix.
; This converges faster than if you dont have a good
; covariance matrix but a little slower than if you have
; a good one.  It is a good choice for an initial run
; to get a good covmat.  In fact, that is how we made
; demos/covmat5.txt

[output]
filename = o3_sim_test.txt
format = text
verbosity= debug


[pipeline]
modules = consistency camb gwem
;values = values-basic.ini
values = values.ini
extra_output =   
likelihoods = gwem
quiet=T
debug=F
timing=F


; To use the maxlike sampler instead you can 
; set sampler=maxlike at the top
[maxlike]
; The values file we use in this example was
; created just like the one we made in demo4,
; by setting sampler = maxlike above, so that
; these settings were used:
output_ini = values_new.ini
; A covariance can only be output by some 
; optimization methods (those that find an approximation to it numerically)
output_covmat = new.cov
tolerance = 1e-6
; The BFGS method seems to find it a bit harder to actually locate
; the peak, but once it is there it provides you with covariance
; matrix estimate
;method = Nelder-Mead
;method = BFGS
; Any minimizer available in scipy can be specified here - they are:
; Nelder-Mead
; Powell
; CG
; BFGS
; Newton-CG
; L-BFGS-B
; TNC
; COBYLA
; SLSQP
; dogleg
; trust-ncg

[consistency]
file = cosmosis-standard-library/utility/consistency/consistency_interface.py

[camb]
file = cosmosis-standard-library/boltzmann/camb/camb.so
mode=background
feedback=0
zmin=0.0
zmax=0.2
nz=100

[gwem]
file = gw/gwem.py
z_frame = cmb
;z_frame = helio
data_dir = data
;data_file = gw170817.txt
;data_file = gw170814_redmapper.txt
;data_file = gw170817_sym.txt
;data_file = gw170817-holz.txt
data_file = test.txt

